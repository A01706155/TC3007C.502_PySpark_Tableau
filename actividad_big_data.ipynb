{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f80046",
   "metadata": {},
   "source": [
    "# Manolo Ramírez Pintor - A01706155\n",
    "## Módulo Big Data:\n",
    "### Utilización, procesamiento y visualización de grandes volúmenes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e69bb0",
   "metadata": {},
   "source": [
    "## 1. Configurando el entorno de trabajo PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bcd84",
   "metadata": {},
   "source": [
    "Iniciamos revisando los recursos que tenemos disponibles en el sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2077f80",
   "metadata": {},
   "source": [
    "Tenemos aproximadamente medio GB de RAM utilizado de los 24 GB que tenemos disponibles en total, así que tenemos suficientes recursos para trabajar con un dataset grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90555ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          23988       11954        6001        5024        6031        6733\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ce47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc5e3b",
   "metadata": {},
   "source": [
    "Para evitar tener muchos mensajes de advertencia en el notebook, importamos warnings para filtrarlos todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77381e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bd9ed",
   "metadata": {},
   "source": [
    "Ahora, ponemos las rutas de donde tenemos instalado Java 8 y Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ad8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estableciendo variable de entorno\n",
    "import os\n",
    "# import pandas as pd\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-arm64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mc/spark/spark-3.2.2-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40a4e4",
   "metadata": {},
   "source": [
    "Importamos findspark e inicializamos la instalación de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545e92cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mc/spark/spark-3.2.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Buscando e inicializando la instalación de Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdf385",
   "metadata": {},
   "source": [
    "Ahora importamos SparkSession y creamos una sesión para este trabajo, en mi caso se llama ``bigData_Manolo``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f494cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/29 03:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fox-arm.subnet02281739.vcn02281739.oraclevcn.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bigData_Manolo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff581e0490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.appName('bigData_Manolo').getOrCreate()\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5607f4",
   "metadata": {},
   "source": [
    "## 2. Seleccionando un dataset de gran tamaño\n",
    "En mi caso, encontré y seleccioné el dataset de [Car Sales](https://www.kaggle.com/datasets/ekibee/car-sales-information), este lo encontré en Kaggle usando los filtros de tamaño. El peso del dataset descomprimido llega a 2GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129257",
   "metadata": {},
   "source": [
    "## 3. Generando un modelo inteligente de regresión con MLlib\n",
    "### a) EDA básico\n",
    "Ya que en Kaggle se indica que los datos no están completamente limpios, voy a revisar qué columnas tienen datos faltantes y qué es lo que puede servirme para luego hacer ETL y comenzar a realizar predicciones.   \n",
    "\n",
    "Al momento de cargar el dataset, podemos ver que existen distintas columnas. Estos son datos obtenidos de una página parecida a un Mercado Libre ruso de automóviles.   \n",
    "\n",
    "Tenemos columnas como la marca, el modelo, el tipo de carrocería, color, tipo de combustible, año, kilometraje, tipo de transmisión, poder en caballos de fuerza, precio en rublos, el nombre del motor, la capacidad del motor (en litros), la fecha de publicación, la ubicación del automóvil, el link de la publicación, la descripción y el momento en el que se capturó la información desde la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7773a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'name',\n",
       " 'bodyType',\n",
       " 'color',\n",
       " 'fuelType',\n",
       " 'year',\n",
       " 'mileage',\n",
       " 'transmission',\n",
       " 'power',\n",
       " 'price',\n",
       " 'vehicleConfiguration',\n",
       " 'engineName',\n",
       " 'engineDisplacement',\n",
       " 'date',\n",
       " 'location',\n",
       " 'link',\n",
       " 'description',\n",
       " 'parse_date']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark_session.read.option(\"header\",True).csv('region25.csv')\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12627bc2",
   "metadata": {},
   "source": [
    "En base a la información que encontré en Kaggle, estableceremos el tipo de dato por columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da5b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"date\",df_spark.date.cast('string'))\n",
    "df_spark = df_spark.withColumn(\"parse_date\",df_spark.parse_date.cast('string'))\n",
    "df_spark = df_spark.withColumn(\"year\",df_spark.year.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"mileage\",df_spark.mileage.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"power\",df_spark.power.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"price\",df_spark.price.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09d4e7",
   "metadata": {},
   "source": [
    "A continuación realizaré un describe, aparecerá roto por el gran número de columnas pero lo arreglaré con Markdown..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8212c",
   "metadata": {},
   "source": [
    "* Tenemos 1,513,200 filas totales en el dataset.   \n",
    "* El precio promedio de los automóviles es de 1,368,558.3 rublos.   \n",
    "* Hay columnas que son de datos numéricos pero que presentan valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84355b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|summary|  brand|              name|     bodyType|  color|fuelType|              year|           mileage|transmission|            power|             price|vehicleConfiguration| engineName|engineDisplacement|               date|   location|                link|         description|          parse_date|\n",
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|  count|1513200|           1513200|      1513200|1403466| 1509640|           1102226|           1498720|     1510135|          1492313|           1513200|             1102226|    1101142|           1092435|            1513200|    1513200|             1513200|             1477463|             1513200|\n",
      "|   mean|   null|1773.5805008944544|         null|   null|    null|2010.3404338130292|134250.93212874988|        null|145.8111408263548| 1368558.300035686|  11.250605168465825|       null|              null|               null|       null|                null|     8.9146877521E10|  3.3098591549295775|\n",
      "| stddev|   null|1015.7212254077098|         null|   null|    null| 7.568867638264089| 85203.82544809658|        null|70.08857549984492|1573677.1363237544|   60.40935143592945|       null|              null|               null|       null|                null|                 0.0|  2.0669060668646346|\n",
      "|    min|  Acura|          1-Series|   Джип 3 дв.|Бежевый|  Бензин|              1943|              1000|        АКПП|                9|             15000| 1.0 CILQ G Packa...|       10HM|           0.5 LTR|2022-08-19 00:00:00|    Анучино|https://anuchino....|! ! ! ПЕРЕКУПОВ П...|  ОТЛИЧНОЕ СОСТОЯ...|\n",
      "|    max|    УАЗ|            Хантер|Хэтчбек 5 дв.| Черный| Электро|              2022|           1000000|       Робот|             1000|          41500000|           ГАЗ-М-21Л|УМЗ-4218.10|           6.4 LTR|2022-09-26 00:00:00|Ярославский|https://zarubino....|                  ……|чем у автомата.Ав...|\n",
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9545",
   "metadata": {},
   "source": [
    "Ahora veremos si las columnas tienen el tipo de dato correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75322b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand , string\n",
      "name , string\n",
      "bodyType , string\n",
      "color , string\n",
      "fuelType , string\n",
      "year , int\n",
      "mileage , int\n",
      "transmission , string\n",
      "power , int\n",
      "price , int\n",
      "vehicleConfiguration , string\n",
      "engineName , string\n",
      "engineDisplacement , string\n",
      "date , string\n",
      "location , string\n",
      "link , string\n",
      "description , string\n",
      "parse_date , string\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el tipo de dato de las columnas\n",
    "for col in df_spark.dtypes:\n",
    "    print(col[0]+\" , \"+col[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5112b",
   "metadata": {},
   "source": [
    "Ya que ahora los tipos de dato son correctos, procederemos a ver qué columnas presentan valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52a9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "|brand|name|bodyType|color |fuelType|year  |mileage|transmission|power|price|vehicleConfiguration|engineName|engineDisplacement|date|location|link|description|parse_date|\n",
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "|0    |0   |0       |109734|3560    |410974|14480  |3065        |20887|0    |410974              |412058    |420765            |0   |0       |0   |35737      |0         |\n",
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora contaremos los valores nulos con isnan, when, count y col\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94311700",
   "metadata": {},
   "source": [
    "Como la tabla no sale bien, pondré manualmente qué columnas presentan valores nulos y cuántos:\n",
    "* Color: 109,734\n",
    "* fuelType: 3,560\n",
    "* Year: 410,974\n",
    "* Mileage: 14,480\n",
    "* Transmission: 3,065\n",
    "* Power: 20,887\n",
    "* vehicleConfiguration: 410,974\n",
    "* engineName: 412,058\n",
    "* engineDisplacement: 420,765\n",
    "* description: 35,737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ed22b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "|brand|name|bodyType|color|fuelType|year|mileage|transmission|power|price|vehicleConfiguration|engineName|engineDisplacement|date|location| link|description|parse_date|\n",
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "|   74|1026|      11|   16|       3|  59|    541|           5|  352| 2986|                7955|      1149|                55|  39|      71|50119|      63606|      2521|\n",
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora para obtener los valores únicos, utilizaremos count_distincst\n",
    "from pyspark.sql.functions import count_distinct\n",
    "\n",
    "# Contaremos los valores únicos de cada columna\n",
    "df_spark.select([count_distinct(c).alias(c) for c in df_spark.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72c658",
   "metadata": {},
   "source": [
    "Como la tabla no sale bien, pondré manualmente los valores únicos de cada columna:\n",
    "* brand:\t67\n",
    "* name:\t884\n",
    "* bodyType:\t11\n",
    "* color:\t16\n",
    "* fuelType:\t3\n",
    "* year:\t54\n",
    "* mileage:\t483\n",
    "* transmission:\t5\n",
    "* power:\t323\n",
    "* price:\t2037\n",
    "* vehicleConfiguration:\t5620\n",
    "* engineName:\t897\n",
    "* engineDisplacement:\t53\n",
    "* date:\t16\n",
    "* location:\t69\n",
    "* link:\t24757\n",
    "* description:\t28790\n",
    "* parse_date:\t307"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5edf6",
   "metadata": {},
   "source": [
    "### b) ETL\n",
    "Viendo un poco las columnas que tenemos y las descripciones cortas obtenidas de Kaggle, tenemos información que **no nos va a servir de inicio**, como ``parse_date``, ``description``, ``link``, ``location``, ``date``, ``engineDisplacement``, ``engineName``, ``vehicleConfiguration`` y ``year``.   \n",
    "\n",
    "La justificación de quitarlos es que presentan una gran cantidad de datos nulos, no son relevantes para la venta de un automóvil, existen automóviles viejos que pueden ser muy baratos y muy caros a la vez e incluso hay datos que son únicos por registro. Entonces procederemos a hacer un drop de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2c86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop('year','vehicleConfiguration','engineName',\n",
    "                         'engineDisplacement', 'link','description',\n",
    "                         'parse_date','date','location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2176bf",
   "metadata": {},
   "source": [
    "Revisamos las columnas que nos quedan, ahora podremos trabajar mejor con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b41078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'name',\n",
       " 'bodyType',\n",
       " 'color',\n",
       " 'fuelType',\n",
       " 'mileage',\n",
       " 'transmission',\n",
       " 'power',\n",
       " 'price']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7ee23",
   "metadata": {},
   "source": [
    "Procederemos a revisar las columnas con pocos valores únicos y que coincidan con los nulos para ver lo que contienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f7c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(color='Бордовый'),\n",
       " Row(color='Белый'),\n",
       " Row(color='Золотистый'),\n",
       " Row(color='Коричневый'),\n",
       " Row(color=None),\n",
       " Row(color='Оранжевый'),\n",
       " Row(color='Серебристый'),\n",
       " Row(color='Розовый'),\n",
       " Row(color='Фиолетовый'),\n",
       " Row(color='Бежевый'),\n",
       " Row(color='Красный'),\n",
       " Row(color='Голубой'),\n",
       " Row(color='Серый'),\n",
       " Row(color='Желтый'),\n",
       " Row(color='Зеленый'),\n",
       " Row(color='Черный'),\n",
       " Row(color='Синий')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos los colores de los automóviles\n",
    "df_spark.select('color').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3067eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(fuelType=None),\n",
       " Row(fuelType='Дизель'),\n",
       " Row(fuelType='Электро'),\n",
       " Row(fuelType='Бензин')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos los tipos de combustión de los automóviles\n",
    "df_spark.select('fuelType').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e3d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(transmission='Механика'),\n",
       " Row(transmission='Робот'),\n",
       " Row(transmission=None),\n",
       " Row(transmission='Вариатор'),\n",
       " Row(transmission='Автомат'),\n",
       " Row(transmission='АКПП')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transmisión\n",
    "df_spark.select('transmission').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b480676",
   "metadata": {},
   "source": [
    "Después de un análisis rápido, pude ver que es posible generalizar datos nulos en vez de borrarlos directamente y comenzar a perder información.   \n",
    "\n",
    "Por ejemplo, el color más común de los automóviles es blanco y podemos partir de ahí para rellenar los strings nulos o vacíos con ``Белый``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33fe8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('Белый', 'color')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c35fba",
   "metadata": {},
   "source": [
    "Ahora, el tipo de combustible que más se ues es la gasolina, siendo la más común de los automóviles, así que llenaré el tipo de combustible con ``Бензин``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aab9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('Бензин', 'fuelType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76b425",
   "metadata": {},
   "source": [
    "Revisando el progreso de los valores nulos, ahora sólo quedan las columnas de kilometraje, poder y transmisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c12401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|brand|name|bodyType|color|fuelType|mileage|transmission|power|price|\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|0    |0   |0       |0    |0       |14480  |3065        |20887|0    |\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dfb13",
   "metadata": {},
   "source": [
    "Las columnas de poder y kilometraje las podemos reemplazar con valores de la media, el promedio o la moda ya que son columnas con el tipo de dato entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f946f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['power', 'mileage'],\n",
    "    outputCols = ['power', 'mileage']\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3842a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96d8c6",
   "metadata": {},
   "source": [
    "Quizá sea bueno poner los tipos de transmisión faltantes con la moda por esta ocasión.   \n",
    "\n",
    "Usaré consultas SQL ya que no encontré otra manera de hallar la moda y el Imputer tristemente me da error si trato de hacerlo con tipo de dato de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b648aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|transmission| count|\n",
      "+------------+------+\n",
      "|    Вариатор|677023|\n",
      "|        АКПП|645355|\n",
      "|    Механика| 73331|\n",
      "|     Автомат| 59233|\n",
      "|       Робот| 55193|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux = df_spark.where(col('transmission').isNotNull())\n",
    "\n",
    "df_aux.createOrReplaceTempView('table')\n",
    "df_aux_2 = spark_session.sql(\n",
    "    'SELECT transmission, COUNT(transmission) AS count FROM table GROUP BY transmission ORDER BY count desc'\n",
    ")\n",
    "\n",
    "df_aux_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf51cc0",
   "metadata": {},
   "source": [
    "Podemos observar que el tipo de transmisión más común es el de CVT, (o transmisión continuamente variable), así que se lo asignaremos a los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4fbd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('Вариатор', 'transmission')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c302509",
   "metadata": {},
   "source": [
    "Ahora revisaré si ya no tenemos valores nulos en nuestro dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a94beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|brand|name|bodyType|color|fuelType|mileage|transmission|power|price|\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|0    |0   |0       |0    |0       |0      |0           |0    |0    |\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00706693",
   "metadata": {},
   "source": [
    "Ya que no tenemos ningún dato nulo y puedo decir que ahora mis datos están limpios, guardaré una copia y la insertaré dentro de Tableau más adelante para observar datos de forma visual y darme una idea de cómo están las cosas. 😀   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc103e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.option(\"header\",true).csv('region25_less.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49b800",
   "metadata": {},
   "source": [
    "### c) Generando el modelo con MLLib\n",
    "Ahora vamos a generar un modelo inteligente de clasificación de regresión con el objetivo de predecir el precio de los automóviles en el mercado libre ruso de automóviles.   \n",
    "\n",
    "Primero importamos las herramientas para entrenar con regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d737f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb24964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ed826",
   "metadata": {},
   "source": [
    "Ahora separamos los datos de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efeb0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df_spark.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f9341",
   "metadata": {},
   "source": [
    "Procedemos a usar la función de regresión linear con gradiente descendiente usando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3595a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LinearRegressionWithSGD.train(sc.parallelize(trainingData), iterations=10,\n",
    "    initialWeights=np.array([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868480d4",
   "metadata": {},
   "source": [
    "## 4. Evaluando el modelo con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383fa97",
   "metadata": {},
   "source": [
    "## 5. Generando un tablero de visualización con Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
