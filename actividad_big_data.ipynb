{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f80046",
   "metadata": {},
   "source": [
    "# Manolo Ram√≠rez Pintor - A01706155\n",
    "## M√≥dulo Big Data:\n",
    "### Utilizaci√≥n, procesamiento y visualizaci√≥n de grandes vol√∫menes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e69bb0",
   "metadata": {},
   "source": [
    "## 1. Configurando el entorno de trabajo PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bcd84",
   "metadata": {},
   "source": [
    "Iniciamos revisando los recursos que tenemos disponibles en el sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2077f80",
   "metadata": {},
   "source": [
    "Tenemos aproximadamente medio GB de RAM utilizado de los 24 GB que tenemos disponibles en total, as√≠ que tenemos suficientes recursos para trabajar con un dataset grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90555ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          23988       11954        6001        5024        6031        6733\r\n",
      "Swap:             0           0           0\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ce47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc5e3b",
   "metadata": {},
   "source": [
    "Para evitar tener muchos mensajes de advertencia en el notebook, importamos warnings para filtrarlos todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77381e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bd9ed",
   "metadata": {},
   "source": [
    "Ahora, ponemos las rutas de donde tenemos instalado Java 8 y Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ad8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estableciendo variable de entorno\n",
    "import os\n",
    "# import pandas as pd\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-arm64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mc/spark/spark-3.2.2-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40a4e4",
   "metadata": {},
   "source": [
    "Importamos findspark e inicializamos la instalaci√≥n de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545e92cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mc/spark/spark-3.2.2-bin-hadoop3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Buscando e inicializando la instalaci√≥n de Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdf385",
   "metadata": {},
   "source": [
    "Ahora importamos SparkSession y creamos una sesi√≥n para este trabajo, en mi caso se llama ``bigData_Manolo``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f494cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/29 03:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fox-arm.subnet02281739.vcn02281739.oraclevcn.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bigData_Manolo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff581e0490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.appName('bigData_Manolo').getOrCreate()\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5607f4",
   "metadata": {},
   "source": [
    "## 2. Seleccionando un dataset de gran tama√±o\n",
    "En mi caso, encontr√© y seleccion√© el dataset de [Car Sales](https://www.kaggle.com/datasets/ekibee/car-sales-information), este lo encontr√© en Kaggle usando los filtros de tama√±o. El peso del dataset descomprimido llega a 2GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129257",
   "metadata": {},
   "source": [
    "## 3. Generando un modelo inteligente de regresi√≥n con MLlib\n",
    "### a) EDA b√°sico\n",
    "Ya que en Kaggle se indica que los datos no est√°n completamente limpios, voy a revisar qu√© columnas tienen datos faltantes y qu√© es lo que puede servirme para luego hacer ETL y comenzar a realizar predicciones.   \n",
    "\n",
    "Al momento de cargar el dataset, podemos ver que existen distintas columnas. Estos son datos obtenidos de una p√°gina parecida a un Mercado Libre ruso de autom√≥viles.   \n",
    "\n",
    "Tenemos columnas como la marca, el modelo, el tipo de carrocer√≠a, color, tipo de combustible, a√±o, kilometraje, tipo de transmisi√≥n, poder en caballos de fuerza, precio en rublos, el nombre del motor, la capacidad del motor (en litros), la fecha de publicaci√≥n, la ubicaci√≥n del autom√≥vil, el link de la publicaci√≥n, la descripci√≥n y el momento en el que se captur√≥ la informaci√≥n desde la p√°gina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7773a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'name',\n",
       " 'bodyType',\n",
       " 'color',\n",
       " 'fuelType',\n",
       " 'year',\n",
       " 'mileage',\n",
       " 'transmission',\n",
       " 'power',\n",
       " 'price',\n",
       " 'vehicleConfiguration',\n",
       " 'engineName',\n",
       " 'engineDisplacement',\n",
       " 'date',\n",
       " 'location',\n",
       " 'link',\n",
       " 'description',\n",
       " 'parse_date']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark_session.read.option(\"header\",True).csv('region25.csv')\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12627bc2",
   "metadata": {},
   "source": [
    "En base a la informaci√≥n que encontr√© en Kaggle, estableceremos el tipo de dato por columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da5b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"date\",df_spark.date.cast('string'))\n",
    "df_spark = df_spark.withColumn(\"parse_date\",df_spark.parse_date.cast('string'))\n",
    "df_spark = df_spark.withColumn(\"year\",df_spark.year.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"mileage\",df_spark.mileage.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"power\",df_spark.power.cast('int'))\n",
    "df_spark = df_spark.withColumn(\"price\",df_spark.price.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09d4e7",
   "metadata": {},
   "source": [
    "A continuaci√≥n realizar√© un describe, aparecer√° roto por el gran n√∫mero de columnas pero lo arreglar√© con Markdown..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8212c",
   "metadata": {},
   "source": [
    "* Tenemos 1,513,200 filas totales en el dataset.   \n",
    "* El precio promedio de los autom√≥viles es de 1,368,558.3 rublos.   \n",
    "* Hay columnas que son de datos num√©ricos pero que presentan valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84355b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|summary|  brand|              name|     bodyType|  color|fuelType|              year|           mileage|transmission|            power|             price|vehicleConfiguration| engineName|engineDisplacement|               date|   location|                link|         description|          parse_date|\n",
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|  count|1513200|           1513200|      1513200|1403466| 1509640|           1102226|           1498720|     1510135|          1492313|           1513200|             1102226|    1101142|           1092435|            1513200|    1513200|             1513200|             1477463|             1513200|\n",
      "|   mean|   null|1773.5805008944544|         null|   null|    null|2010.3404338130292|134250.93212874988|        null|145.8111408263548| 1368558.300035686|  11.250605168465825|       null|              null|               null|       null|                null|     8.9146877521E10|  3.3098591549295775|\n",
      "| stddev|   null|1015.7212254077098|         null|   null|    null| 7.568867638264089| 85203.82544809658|        null|70.08857549984492|1573677.1363237544|   60.40935143592945|       null|              null|               null|       null|                null|                 0.0|  2.0669060668646346|\n",
      "|    min|  Acura|          1-Series|   –î–∂–∏–ø 3 –¥–≤.|–ë–µ–∂–µ–≤—ã–π|  –ë–µ–Ω–∑–∏–Ω|              1943|              1000|        –ê–ö–ü–ü|                9|             15000| 1.0 CILQ G Packa...|       10HM|           0.5 LTR|2022-08-19 00:00:00|    –ê–Ω—É—á–∏–Ω–æ|https://anuchino....|! ! ! –ü–ï–†–ï–ö–£–ü–û–í –ü...|  –û–¢–õ–ò–ß–ù–û–ï –°–û–°–¢–û–Ø...|\n",
      "|    max|    –£–ê–ó|            –•–∞–Ω—Ç–µ—Ä|–•—ç—Ç—á–±–µ–∫ 5 –¥–≤.| –ß–µ—Ä–Ω—ã–π| –≠–ª–µ–∫—Ç—Ä–æ|              2022|           1000000|       –†–æ–±–æ—Ç|             1000|          41500000|           –ì–ê–ó-–ú-21–õ|–£–ú–ó-4218.10|           6.4 LTR|2022-09-26 00:00:00|–Ø—Ä–æ—Å–ª–∞–≤—Å–∫–∏–π|https://zarubino....|                  ‚Ä¶‚Ä¶|—á–µ–º —É –∞–≤—Ç–æ–º–∞—Ç–∞.–ê–≤...|\n",
      "+-------+-------+------------------+-------------+-------+--------+------------------+------------------+------------+-----------------+------------------+--------------------+-----------+------------------+-------------------+-----------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a9545",
   "metadata": {},
   "source": [
    "Ahora veremos si las columnas tienen el tipo de dato correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75322b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand , string\n",
      "name , string\n",
      "bodyType , string\n",
      "color , string\n",
      "fuelType , string\n",
      "year , int\n",
      "mileage , int\n",
      "transmission , string\n",
      "power , int\n",
      "price , int\n",
      "vehicleConfiguration , string\n",
      "engineName , string\n",
      "engineDisplacement , string\n",
      "date , string\n",
      "location , string\n",
      "link , string\n",
      "description , string\n",
      "parse_date , string\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos el tipo de dato de las columnas\n",
    "for col in df_spark.dtypes:\n",
    "    print(col[0]+\" , \"+col[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5112b",
   "metadata": {},
   "source": [
    "Ya que ahora los tipos de dato son correctos, procederemos a ver qu√© columnas presentan valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52a9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "|brand|name|bodyType|color |fuelType|year  |mileage|transmission|power|price|vehicleConfiguration|engineName|engineDisplacement|date|location|link|description|parse_date|\n",
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "|0    |0   |0       |109734|3560    |410974|14480  |3065        |20887|0    |410974              |412058    |420765            |0   |0       |0   |35737      |0         |\n",
      "+-----+----+--------+------+--------+------+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora contaremos los valores nulos con isnan, when, count y col\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94311700",
   "metadata": {},
   "source": [
    "Como la tabla no sale bien, pondr√© manualmente qu√© columnas presentan valores nulos y cu√°ntos:\n",
    "* Color: 109,734\n",
    "* fuelType: 3,560\n",
    "* Year: 410,974\n",
    "* Mileage: 14,480\n",
    "* Transmission: 3,065\n",
    "* Power: 20,887\n",
    "* vehicleConfiguration: 410,974\n",
    "* engineName: 412,058\n",
    "* engineDisplacement: 420,765\n",
    "* description: 35,737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ed22b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "|brand|name|bodyType|color|fuelType|year|mileage|transmission|power|price|vehicleConfiguration|engineName|engineDisplacement|date|location| link|description|parse_date|\n",
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "|   74|1026|      11|   16|       3|  59|    541|           5|  352| 2986|                7955|      1149|                55|  39|      71|50119|      63606|      2521|\n",
      "+-----+----+--------+-----+--------+----+-------+------------+-----+-----+--------------------+----------+------------------+----+--------+-----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora para obtener los valores √∫nicos, utilizaremos count_distincst\n",
    "from pyspark.sql.functions import count_distinct\n",
    "\n",
    "# Contaremos los valores √∫nicos de cada columna\n",
    "df_spark.select([count_distinct(c).alias(c) for c in df_spark.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72c658",
   "metadata": {},
   "source": [
    "Como la tabla no sale bien, pondr√© manualmente los valores √∫nicos de cada columna:\n",
    "* brand:\t67\n",
    "* name:\t884\n",
    "* bodyType:\t11\n",
    "* color:\t16\n",
    "* fuelType:\t3\n",
    "* year:\t54\n",
    "* mileage:\t483\n",
    "* transmission:\t5\n",
    "* power:\t323\n",
    "* price:\t2037\n",
    "* vehicleConfiguration:\t5620\n",
    "* engineName:\t897\n",
    "* engineDisplacement:\t53\n",
    "* date:\t16\n",
    "* location:\t69\n",
    "* link:\t24757\n",
    "* description:\t28790\n",
    "* parse_date:\t307"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5edf6",
   "metadata": {},
   "source": [
    "### b) ETL\n",
    "Viendo un poco las columnas que tenemos y las descripciones cortas obtenidas de Kaggle, tenemos informaci√≥n que **no nos va a servir de inicio**, como ``parse_date``, ``description``, ``link``, ``location``, ``date``, ``engineDisplacement``, ``engineName``, ``vehicleConfiguration`` y ``year``.   \n",
    "\n",
    "La justificaci√≥n de quitarlos es que presentan una gran cantidad de datos nulos, no son relevantes para la venta de un autom√≥vil, existen autom√≥viles viejos que pueden ser muy baratos y muy caros a la vez e incluso hay datos que son √∫nicos por registro. Entonces procederemos a hacer un drop de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2c86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop('year','vehicleConfiguration','engineName',\n",
    "                         'engineDisplacement', 'link','description',\n",
    "                         'parse_date','date','location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2176bf",
   "metadata": {},
   "source": [
    "Revisamos las columnas que nos quedan, ahora podremos trabajar mejor con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b41078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brand',\n",
       " 'name',\n",
       " 'bodyType',\n",
       " 'color',\n",
       " 'fuelType',\n",
       " 'mileage',\n",
       " 'transmission',\n",
       " 'power',\n",
       " 'price']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7ee23",
   "metadata": {},
   "source": [
    "Procederemos a revisar las columnas con pocos valores √∫nicos y que coincidan con los nulos para ver lo que contienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f7c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(color='–ë–æ—Ä–¥–æ–≤—ã–π'),\n",
       " Row(color='–ë–µ–ª—ã–π'),\n",
       " Row(color='–ó–æ–ª–æ—Ç–∏—Å—Ç—ã–π'),\n",
       " Row(color='–ö–æ—Ä–∏—á–Ω–µ–≤—ã–π'),\n",
       " Row(color=None),\n",
       " Row(color='–û—Ä–∞–Ω–∂–µ–≤—ã–π'),\n",
       " Row(color='–°–µ—Ä–µ–±—Ä–∏—Å—Ç—ã–π'),\n",
       " Row(color='–†–æ–∑–æ–≤—ã–π'),\n",
       " Row(color='–§–∏–æ–ª–µ—Ç–æ–≤—ã–π'),\n",
       " Row(color='–ë–µ–∂–µ–≤—ã–π'),\n",
       " Row(color='–ö—Ä–∞—Å–Ω—ã–π'),\n",
       " Row(color='–ì–æ–ª—É–±–æ–π'),\n",
       " Row(color='–°–µ—Ä—ã–π'),\n",
       " Row(color='–ñ–µ–ª—Ç—ã–π'),\n",
       " Row(color='–ó–µ–ª–µ–Ω—ã–π'),\n",
       " Row(color='–ß–µ—Ä–Ω—ã–π'),\n",
       " Row(color='–°–∏–Ω–∏–π')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos los colores de los autom√≥viles\n",
    "df_spark.select('color').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3067eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(fuelType=None),\n",
       " Row(fuelType='–î–∏–∑–µ–ª—å'),\n",
       " Row(fuelType='–≠–ª–µ–∫—Ç—Ä–æ'),\n",
       " Row(fuelType='–ë–µ–Ω–∑–∏–Ω')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos los tipos de combusti√≥n de los autom√≥viles\n",
    "df_spark.select('fuelType').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e3d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(transmission='–ú–µ—Ö–∞–Ω–∏–∫–∞'),\n",
       " Row(transmission='–†–æ–±–æ—Ç'),\n",
       " Row(transmission=None),\n",
       " Row(transmission='–í–∞—Ä–∏–∞—Ç–æ—Ä'),\n",
       " Row(transmission='–ê–≤—Ç–æ–º–∞—Ç'),\n",
       " Row(transmission='–ê–ö–ü–ü')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transmisi√≥n\n",
    "df_spark.select('transmission').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b480676",
   "metadata": {},
   "source": [
    "Despu√©s de un an√°lisis r√°pido, pude ver que es posible generalizar datos nulos en vez de borrarlos directamente y comenzar a perder informaci√≥n.   \n",
    "\n",
    "Por ejemplo, el color m√°s com√∫n de los autom√≥viles es blanco y podemos partir de ah√≠ para rellenar los strings nulos o vac√≠os con ``–ë–µ–ª—ã–π``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33fe8e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('–ë–µ–ª—ã–π', 'color')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c35fba",
   "metadata": {},
   "source": [
    "Ahora, el tipo de combustible que m√°s se ues es la gasolina, siendo la m√°s com√∫n de los autom√≥viles, as√≠ que llenar√© el tipo de combustible con ``–ë–µ–Ω–∑–∏–Ω``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aab9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('–ë–µ–Ω–∑–∏–Ω', 'fuelType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f76b425",
   "metadata": {},
   "source": [
    "Revisando el progreso de los valores nulos, ahora s√≥lo quedan las columnas de kilometraje, poder y transmisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c12401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|brand|name|bodyType|color|fuelType|mileage|transmission|power|price|\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|0    |0   |0       |0    |0       |14480  |3065        |20887|0    |\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dfb13",
   "metadata": {},
   "source": [
    "Las columnas de poder y kilometraje las podemos reemplazar con valores de la media, el promedio o la moda ya que son columnas con el tipo de dato entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f946f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['power', 'mileage'],\n",
    "    outputCols = ['power', 'mileage']\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3842a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = imputer.fit(df_spark).transform(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed96d8c6",
   "metadata": {},
   "source": [
    "Quiz√° sea bueno poner los tipos de transmisi√≥n faltantes con la moda por esta ocasi√≥n.   \n",
    "\n",
    "Usar√© consultas SQL ya que no encontr√© otra manera de hallar la moda y el Imputer tristemente me da error si trato de hacerlo con tipo de dato de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b648aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|transmission| count|\n",
      "+------------+------+\n",
      "|    –í–∞—Ä–∏–∞—Ç–æ—Ä|677023|\n",
      "|        –ê–ö–ü–ü|645355|\n",
      "|    –ú–µ—Ö–∞–Ω–∏–∫–∞| 73331|\n",
      "|     –ê–≤—Ç–æ–º–∞—Ç| 59233|\n",
      "|       –†–æ–±–æ—Ç| 55193|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux = df_spark.where(col('transmission').isNotNull())\n",
    "\n",
    "df_aux.createOrReplaceTempView('table')\n",
    "df_aux_2 = spark_session.sql(\n",
    "    'SELECT transmission, COUNT(transmission) AS count FROM table GROUP BY transmission ORDER BY count desc'\n",
    ")\n",
    "\n",
    "df_aux_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf51cc0",
   "metadata": {},
   "source": [
    "Podemos observar que el tipo de transmisi√≥n m√°s com√∫n es el de CVT, (o transmisi√≥n continuamente variable), as√≠ que se lo asignaremos a los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4fbd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.na.fill('–í–∞—Ä–∏–∞—Ç–æ—Ä', 'transmission')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c302509",
   "metadata": {},
   "source": [
    "Ahora revisar√© si ya no tenemos valores nulos en nuestro dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a94beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|brand|name|bodyType|color|fuelType|mileage|transmission|power|price|\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "|0    |0   |0       |0    |0       |0      |0           |0    |0    |\n",
      "+-----+----+--------+-----+--------+-------+------------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00706693",
   "metadata": {},
   "source": [
    "Ya que no tenemos ning√∫n dato nulo y puedo decir que ahora mis datos est√°n limpios, guardar√© una copia y la insertar√© dentro de Tableau m√°s adelante para observar datos de forma visual y darme una idea de c√≥mo est√°n las cosas. üòÄ   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc103e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.option(\"header\",true).csv('region25_less.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49b800",
   "metadata": {},
   "source": [
    "### c) Generando el modelo con MLLib\n",
    "Ahora vamos a generar un modelo inteligente de clasificaci√≥n de regresi√≥n con el objetivo de predecir el precio de los autom√≥viles en el mercado libre ruso de autom√≥viles.   \n",
    "\n",
    "Primero importamos las herramientas para entrenar con regresi√≥n lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d737f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb24964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ed826",
   "metadata": {},
   "source": [
    "Ahora separamos los datos de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efeb0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df_spark.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f9341",
   "metadata": {},
   "source": [
    "Procedemos a usar la funci√≥n de regresi√≥n linear con gradiente descendiente usando los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3595a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LinearRegressionWithSGD.train(sc.parallelize(trainingData), iterations=10,\n",
    "    initialWeights=np.array([1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868480d4",
   "metadata": {},
   "source": [
    "## 4. Evaluando el modelo con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383fa97",
   "metadata": {},
   "source": [
    "## 5. Generando un tablero de visualizaci√≥n con Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
